batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters θ for the entire training dataset:

θ = θ − η ⋅ ∇_θJ(θ)

As we need to calculate the gradients for the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that don't fit in memory. Batch gradient descent also doesn't allow us to update our model online, i.e. with new examples on-the-fly.

Gradient descent algorithm
Minimize cost function J
  Gradient descent
Start by looking at a general J() function
Problem
We have J(θ0, θ1)
We want to get min J(θ0, θ1)
Gradient descent applies to more general functions
J(θ0, θ1, θ2 .... θn)
min J(θ0, θ1, θ2 .... θn)

 How does it work?
Start with initial guesses
Start at 0,0 (or any other value)
Keeping changing θ0 and θ1 a little bit to try and reduce J(θ0,θ1)
Each time you change the parameters, you select the gradient which reduces J(θ0,θ1) the most possible
Repeat

Do so until you converge to a local minimum
function [theta, costs] = gradientDescent(X, y, theta, alpha, iterations)
    m = length(y);
    costs = zeros(m,1); %// New

    for iter =1:1:iterations
    delta=zeros(2,1); %// Place here
   for i=1:1:m
       delta(1,1)= delta(1,1)+( X(i,:)*theta - y(i,1))  ;
       delta(2,1)=delta(2,1)+ (( X(i,:)*theta - y(i,1))*X(i,2)) ;
   end
    theta= theta-( delta*(alpha/m) );
   costs(iter) = computeCost(X,y,theta); // New
end
end
https://stackoverflow.com/questions/32274474/machine-learning-linear-regression-using-batch-gradient-descent?noredirect=1&lq=1
// X is data matrix composed of m rows corresponding to m training samples and n columns corresponding to n features.
// theta is our learned weight vector from gradient descent with n+1 freatures accounting for the intercept term
// y is 
// to compute X*theta
